{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwbC4EtJFFTC"
   },
   "source": [
    "## **Coletando Dados da Amazon** - Um Estudo de Caso\n",
    "\n",
    "  A Amazon é uma das empresas que saiu, literalmente, de uma garagem, como uma empresa de venda de livros pela internet e se tornou um dos maiores empreendimentos capitalistas da história. Liderada por Jeff Bezos, hoje a empresa acumula uma capitalização de mercado de mais de U$1 trilhão de dólares. [[1]](https://ycharts.com/companies/AMZN/market_cap) \n",
    "\n",
    "  Neste estudo de caso iremos abordar como coletar dados da plataforma da Amazon diretamente por meio de **web scraping**, fazendo uso dos componentes **requests** [[2]](https://requests.readthedocs.io/pt_BR/latest/user/quickstart.html) e **bs4** - Beautiful Soup [[3]](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) Python 3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5B_Qo4GAPGi"
   },
   "source": [
    "\n",
    "## **1. Entendendo o Problema**\n",
    "* **Qual é o nosso caso?**\n",
    "\n",
    "Neste nosso caso, necessitamos consumir os dados apenas da **primeira pagina**, de acordo com uma **palavra-chave**, que no nosso caso é 'IPHONE' e corresponde à variável **keyword**, no codigo.\n",
    "\n",
    "* **O problema das requests**\n",
    "\n",
    "Caso tentemos realizar uma request para o site da Amazon sem o devido *header*, ou cabeçalho, o site nos retorna uma página sem nenhum item de pesquisa. Isso se deve a algum mecanismo interno do próprio site/API que nos impede, afortunadamente, de acessar os dados. Para solucionar isto, fingimos ter um User-Agent diferente daquele o qual o componente **requests** utiliza para capturar os dados de requisições."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux2i4k1RDZdC"
   },
   "source": [
    "## **2. Exploração dos Dados**\n",
    "Os dados que receberemos serão todos do tipo **string**. Estes dados vem do tipo <class 'bs4.element.Tag'>, que corresponde ao tipo retornado pelo plugin **Beautiful Soup** ao se realizar uma busca com o metodo **find**. Conseguimos extrair a string contida dentro desse tipo ao utilizarmos o identificador **.text**. Todos esses valores, ao serem capturados pelo script serão passados para **duas listas**: uma contendo o título de cada **produto** (produtos) e outra contendo o **preco** (precos) de cada produto. Ao final do processamento destes dados, teremos uma lista que coalesce todos esses valores (lista_conteudo) em uma única lista, para então escrevermos os dados em um arquivo .csv.\n",
    "\n",
    "Portanto, teremos os dados:\n",
    "* **produto** - str\n",
    "* **preco** - str\n",
    "* **produtos** - List\n",
    "* **precos** - List\n",
    "* **lista_conteudo** - List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.2.1-py3-none-any.whl (33 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1273 sha256=23bc3badf87fc6166fcba019518a05fe6b624cabc546c6c31d902cbff591a9a4\n",
      "  Stored in directory: c:\\users\\viere\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.9.3 bs4-0.0.1 soupsieve-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W8nfQ7RbEyh0"
   },
   "outputs": [],
   "source": [
    "# Importamos os componentes necessarios\n",
    "import requests\n",
    "import csv as commaseparated\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Aquisicao dos Dados\n",
    "# Definicao do Cabecalho\n",
    "headers = {'User-Agent':'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5LvUs1I_7JO"
   },
   "source": [
    "Neste caso utilizei o **User-Agent** do meu próprio computador para realizar as requisições. Após definirmos as variáveis de aquisição dos dados, podemos prosseguir e realizar a requisição dos dados de acordo com os parâmetros definidos acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "B0rwANK-CUEw"
   },
   "outputs": [],
   "source": [
    "amazon_request = requests.get('https://www.amazon.com.br/s?k=iphone', headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqauLRrUDCBk"
   },
   "source": [
    "## **3. Preparo dos Dados**\n",
    "\n",
    "Receberemos a página inteira ao realizar a requisição, portanto precisamos instanciar a classe BeautifulSoup, que irá transformar o arquivo HTML em uma **árvore complexa** de objetos Python. Após isso, filtramos a árvore para que ela contenha apenas os **resultados de nossa pesquisa**, fazemos isso utilizando o metodo **find**, para que então contenha apenas o span de atributo **data-component-type** igual a **'s-search-results'**, ou seja, o elemento do DOM que contem apenas os resultados de pesquisa.\n",
    "\n",
    "Então, finalmente, normalizamos os resultados encontrados pelo método **find** para uma **string** para então instanciar outra classe BeautifulSoup, que irá conter apenas os elementos desejados - nossos **resultados**.\n",
    "\n",
    "Após isso, declaramos os identificadores dos tipos lista da parte 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iuOj-6gTJ0xf"
   },
   "outputs": [],
   "source": [
    "# Normalizacao dos Dados\n",
    "soup = BeautifulSoup(amazon_request.text, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nRMgcXIDBxZ"
   },
   "source": [
    "## **4. Modelo e Lógica de Processamento de Dados**\n",
    "\n",
    "Apos prepararmos os dados, necessitamos de um modelo para filtrarmos apenas os dados da nossa pesquisa os quais desejamos.\n",
    "\n",
    "## anexa_produtos()\n",
    "Utilizamos do metodo **find_all** para filtrar todos os spans contendo o nome dos produtos (ie. os que contem o atributo **'a-size-base-plus'**) em uma lista chamada **lista_produtos**\n",
    "\n",
    "Para a função **anexa_produtos()**, apenas iteramos sobre o conteúdo da tag **lista_produtos** e os anexamos a lista **produtos**.\n",
    "\n",
    "## anexa_precos()\n",
    "Já para a função **anexa_precos()** a lógica de processamento, devido às circunstâncias, necessita ser mais robusta. Dada a quantidade díspar de valores de precos, implementamos um metodo que busca dentro das divs que contem o atributo **'sg-col-inner'**, correspondente a classe que possui cada produto, em uma lista de nome **lista_div**. A razão da escolha do último filtro sera apresentada a seguir.\n",
    "\n",
    "Ocorrem então três questões:\n",
    "\n",
    "* **Como lidar com produtos que possuem múltiplos preços?**\n",
    "\n",
    "Por exemplo, há produtos que possuem um preço com os descontos incluídos e o preço normal. A nossa opção foi considerar apenas o preço com os descontos incluídos, obviamente.\n",
    "\n",
    "* **Como lidar com produtos que não possuem precos?**\n",
    "\n",
    "Para estes produtos, normalizamos o preço como \"R$ 0,00\", para que não houvesse valores nulos dentro da nossa planilha.\n",
    "\n",
    "* **Como implementar a lógica para filtrar os produtos que possuem apenas um preço daqueles que possuem multiplos preços ou não possuem um preço?**\n",
    "\n",
    "A lógica implementada para lidar com essas duas questões segue no diagrama abaixo.\n",
    "\n",
    "![Fluxo de Controle da funçã anexa_precos()](https://i.imgur.com/PC0u59u.png)\n",
    "\n",
    "## anexa_conteudos()\n",
    "\n",
    "A função anexa_conteudos() apenas itera sobre os valores de cada lista após estes terem sido processados e coalesce os valores em uma lista (lista_conteudo) com todos os pares **[produto, preco]** encontrados. Isto foi necessário para dar mais legibilidade ao código e facilitar a interpretação da função que armazena os dados no arquivo .csv.\n",
    "\n",
    "Segue abaixo o código.\n",
    "\n",
    "## Filtros\n",
    "O filtro **filter_preco_offscreen** corresponde ao filtro dos produtos que possuem preço, independentemente de haver desconto ou não. Estes estão unidos sob o mesmo span, 'a-off-screen'.\n",
    "\n",
    "O filtro **filter_preco_secondary** corresponde ao filtro da classe pai 'a-row a-size-base a-color-secondary' dos produtos que possuem o preço sob o texto 'Mais opções de compra'.\n",
    "\n",
    "O filtro **filter_preco_a_base** corresponde ao filtro do span dentro das classes definidas pelo filter_preco_secondary, ou seja, é o conteúdo do preço dos produtos que são precificados sob o texto 'Mais opções de compra'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "S-2BsXm1UddB"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filter_preco_offscreen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5daa97b552c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0manexa_produtos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0manexa_precos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0manexa_conteudos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Fim da Logica de Processamento de Dados\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-5daa97b552c1>\u001b[0m in \u001b[0;36manexa_precos\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlista_div\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"R$\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlista_div\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mlista_div\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter_preco_offscreen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m                 \u001b[0mprecos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlista_div\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a-price'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'span'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a-price-whole'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filter_preco_offscreen' is not defined"
     ]
    }
   ],
   "source": [
    "# Inicio da Logica de Processamento de Dados\n",
    "lista_produtos = soup.body.find_all('span', attrs={'class':'a-size-base-plus'})\n",
    "lista_div = soup.body.find_all('div', attrs={'class':'sg-col-inner'})\n",
    "filter_preco = {'class': 'a-price-whole'}\n",
    "produtos = []\n",
    "precos = []\n",
    "lista_conteudo = []\n",
    "\n",
    "def anexa_produtos():\n",
    "    for produto in lista_produtos:\n",
    "        produtos.append(produto.text)\n",
    "\n",
    "def anexa_precos():\n",
    "    for i in range(len(lista_div)-1):\n",
    "        if \"R$\" in lista_div[i].text:\n",
    "            if lista_div[i].find('span', attrs=filter_preco_offscreen) is None:\n",
    "                precos.append(lista_div[i].find('span', attrs='a-price').find('span', attrs='a-price-whole').text)\n",
    "            else:\n",
    "                precos.append(lista_div[i].find('span', attrs=filter_preco_offscreen).text)\n",
    "        else:\n",
    "            precos.append(\"R$ 0,00\")\n",
    "\n",
    "def anexa_conteudos():\n",
    "    for produto, preco in zip(produtos, precos):\n",
    "        lista_conteudo.append([produto, preco])\n",
    "        \n",
    "anexa_produtos()\n",
    "anexa_precos()\n",
    "anexa_conteudos()\n",
    "# Fim da Logica de Processamento de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7UrpP7nXeDC"
   },
   "source": [
    "## **5. Armazenamento dos Dados**\n",
    "\n",
    "Para o armazenamento dos dados, escolhemos o tipo **CSV (Comma-separated Values)** por se tratar de um tipo de arquivo universal, que, ao ser aberto em qualquer editor de texto - incluindo-se nisto o **MS Excel** - tem seus dados exibidos de forma padronizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "uFAkc_SGYaYu",
    "outputId": "e5b95126-a726-414a-fc58-3966eae2422d"
   },
   "outputs": [],
   "source": [
    "# Armazenamento dos dados em um arquivo .csv\n",
    "def salva_em_csv():\n",
    "    with open('produtos_amazon.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        spamwriter = commaseparated.writer(csvfile, delimiter= ',')\n",
    "        spamwriter.writerow([\"Produto\", \"Preco\"])\n",
    "        for produto, preco in lista_conteudo:\n",
    "            spamwriter.writerow([produto, preco])\n",
    "            \n",
    "salva_em_csv()\n",
    "files.download('produtos_amazon.csv')\n",
    "df = pd.read_csv('produtos_amazon.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQXuufSNYhiH"
   },
   "source": [
    "## **6. Fragilidades Associadas ao Modelo**\n",
    "\n",
    "* **O modelo não reconhece páginas de pesquisa que contem apenas itens ordenados em lista vertical.**\n",
    "\n",
    "Um exemplo disso pode ser observado ao tentar substituir a palavra-chave por algum item mais generico (ie. \"Bucha\".) Por isso, recomenda-se utilizar palavras-chave mais especificas, como, por exemplo, \"Thinkpad\". \n",
    "\n",
    "* **O modelo depende do limiar associado a quantidade de requests permitidas pela Amazon num dado intervalo de tempo.**\n",
    "\n",
    "Devido a isto, torna-se dificil implementar a paginação ou voltamos ao problema das **requests**, discutido na parte 1 - a chance de se receber códigos de resposta 403 ou 503 é grande se fizermos muitas requests em pouco tempo. Seria interessante, entao, utilizar de proxies/SOCKS e espacar as requests em alguns segundos.\n",
    "\n",
    "\n",
    "## **7. Referências**\n",
    "**[1]** - Valor da Capitalização de Mercado da Amazon - https://ycharts.com/companies/AMZN/market_cap\n",
    "\n",
    "**[2]** - Documentação da Biblioteca Requests - https://requests.readthedocs.io/pt_BR/latest/user/quickstart.html\n",
    "\n",
    "**[3]** - Documentação da Biblioteca bs4 - https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Amazon.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
